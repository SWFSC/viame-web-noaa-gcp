{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VIAME-Web-NOAA-GCP Documentation","text":"<p>The site details how to deploy and manage an instance of VIAME-Web on a NOAA Fisheries-hardened virtual machine (VM) in a GCP Project within the NOAA Fisheries Cloud. </p> <p>These instructions were developed from DIVE Deployment Scenario 1 with the extensive help and expertise of the VIAME team, in particular Brandon Davis and Bryon Lewis, and Ed Rodgers. Note that these docs do not supersede any of the DIVE docs. Rather, they supplement the DIVE deployment options by providing NOAA Fisheries Cloud-specific deployment instructions.</p>"},{"location":"#deploy-your-own-instance","title":"Deploy your own instance","text":"<p>See the Deployment Guide for step-by-step instructions on how to deploy your own instance of VIAME-Web in GCP in the NOAA Fisheries Cloud.</p>"},{"location":"#terminology","title":"Terminology","text":"<p>Per Kitware's concepts and definitions, a deployment of VIAME-Web is a deployment of DIVE Web with associated marine biology-centric pipelines and algorithms. This repo and associated documentation were designed for users deploying the whole VIAME-Web platform, but note that they  could also be used to deploy only an instance of DIVE-Web.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>The United States Department of Commerce (DOC) GitHub project code is provided on an \u2018as is\u2019 basis and the user assumes responsibility for its use. DOC has relinquished control of the information and no longer has responsibility to protect the integrity, confidentiality, or availability of the information. Any claims against the Department of Commerce stemming from the use of its GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government.</p>"},{"location":"admin-docker-autostart/","title":"Starting Docker Containers Automatically","text":"<p>You can configure your deployment so that the necessary docker containers start automatically when a GCP VM is turned on. This avoids the issue (fix here) where other users don't have permissions to run the <code>docker</code> command.</p> <p>One option if you don't want to rely on docker is to use a process manager like systemd for the docker-compose command, and have that configured to run on start-up. This is more powerful as it can be heavily customized, but is more involved to implement.</p> <p>The other option to have the docker containers start automatically is to change the restart policy. You can do this by adding, for instance, <code>restart: unless-stopped</code> to the relevant services. For a default deployment, add the restart policy for each service. For a split deployment you need to add the policy to the traefik, rabbit, mongo, and girder services for the web node, and the girder_worker_default, girder_worker_pipelines, and girder_worker_training services (or just to the base-worker object) for the worker node. For example:</p> <pre><code>...\nservices:\n  traefik:\n    restart: unless-stopped\n    image: traefik:v2.4\n...\n</code></pre> <p>or </p> <pre><code>...\nx-worker: &amp;base-worker\n  deploy:\n    resources:\n      reservations:\n        devices:\n          - capabilities: [gpu]\n  ipc: host\n  build:\n    context: .\n    dockerfile: docker/girder_worker.Dockerfile\n  restart: unless-stopped\n  image: kitware/viame-worker:${TAG:-latest}\n  volumes:\n...\n</code></pre> <p>In the future, this will be added as an option on install. </p> <p>NOTE: if you implement this option, then the docker container updates are not pulled each time the VM is started. You must stop the containers, pull the updates, and spin the containers back up:</p> <p>```bash</p>"},{"location":"admin-docker-autostart/#if-necessary-handle-hardened-vm-permissions","title":"If necessary, handle hardened VM permissions","text":"<p>mkdir $HOMR/tmp #have to run at least once export TMPDIR=$HOME/tmp</p>"},{"location":"admin-docker-autostart/#shut-down-the-containers","title":"Shut down the containers","text":"<p>docker-compose -f /opt/noaa/dive/docker-compose.yml down</p>"},{"location":"admin-docker-autostart/#the-startup-script-pulls-updates-and-starts-the-containers","title":"The startup script pulls updates and starts the containers","text":"<p>/opt/noaa/dive_startup_web.sh #or, eg /opt/noaa/dive_startup_worker.sh ````</p>"},{"location":"admin-general/","title":"General","text":"<p>This page provides a summary of recommended best practices for managing your own deployment of VIAME-Web in GCP in the NOAA Fisheries Cloud. Note that these admin options should be evaluated and only used if appropriate to your specific use case.</p>"},{"location":"admin-general/#addon-management","title":"Addon Management","text":"<p>To download and scan for VIAME addons, such as canned pipelines and algorithms, see the Addon Management docs.</p>"},{"location":"admin-general/#staying-up-to-date","title":"Staying up to date","text":"<p>Keeping your deployment up to date is crucial for getting the latest VIAME-Web features and bug fixes, as well as for debugging purposes. It is strongly recommended that you update your images at least once per week.</p> <p>The startup scripts that are created on your VMs during deployment provisioning include <code>docker-compose pull</code> calls that will update the containers installed on your VM(s) to the <code>latest</code> images. Thus, these containers will be updated every time you run the startup script. Depending on your VM management strategy this may be sufficient, particularly if you turn off your VM(s) at least once per week.</p> <p>If your VM (default or web) will be on at all times, you should consider the Production deployment options.</p> <p>Note that keeping images up to date will not update canned VIAME addons. See Addon Management for details on updating VIAME addons.</p>"},{"location":"admin-general/#data-backup","title":"Data backup","text":"<p>Data in GCS Storage buckets are quite durable. See the GCS Storage docs for more details about data redundancy, versioning, etc.</p> <p>You can create a GCP Snapshot Schedule to create snapshot backups of your disk(s). </p> <p>Click here to download a Terraform code template and usage instructions for data backup and VM management. Copy this code into your Terraform config file (e.g., main.tf) and update project-specific values as needed. In the Cloud Shell Terminal, run <code>terraform init</code>, and then <code>terraform apply</code> to create the resources.</p>"},{"location":"admin-general/#vm-management","title":"VM management","text":"<p>Your VM schedule, i.e., when your VMs are on (and incurring more costs) and when they are off, will be up to your group. You can use the Google Cloud Pricing Calculator to estimate costs.</p> <p>If you have a split services deployment, an example schedule might be leaving the web VM during the work week, with an Instance Schedule set to turn it off on Friday evening, and turning the worker on and off as needed to run jobs. This would minimize users needing to turn on the web VM (or turning it off while another user is annotating), which also only having the worker VM on when necessary. Recent feature additions allow all users to see the status of the job queue so they know if someone else is currently running a job.</p> <p>Click here to download a Terraform code template and usage instructions for data backup and VM management.</p>"},{"location":"admin-general/#server-branding-config","title":"Server Branding Config","text":"<p>For information on how to configure the brand and messaging that appears in various places in the Web UI, see the Server Branding Config docs.</p>"},{"location":"admin-general/#storing-imagery","title":"Storing imagery","text":"<p>Within GCP, storage on Compute Engine disks is much more expensive than storage in GCS buckets. Thus, it is recommended to store your imagery in a GCS bucket and mirror that bucket in your VIAME-Web deployment as a read-only assetstore. See Cloud Storage Integration.</p>"},{"location":"admin-server-migration/","title":"Migrating to a new server","text":"<p>Within GCP you may need to move to a new project, to resize a disk, or to use a different image. However, these actions require destroying and recreating VMs, and destroying a VM and its associated disk means that all data stored on that VM (e.g., user annotations or trained models) will be lost. This section details how to do this without losing any user data (user accounts, annotations, etc.).</p>"},{"location":"admin-server-migration/#default-and-web-vm","title":"Default and Web VM","text":"<p>The default and web VMs both store user data, and thus care should be taken before destroying these VMs. If these resources do need to be destroyed and recreated, e.g., to move from a Dev to a Prod project, then you can either a) restore a snapshot or b) follow the instructions below to avoid losing any user data.</p> <p>Migrating to a different server should be as easy as copying a few directories from the web server node (web VM).  Everything is in /var/lib/docker/volumes (assuming you haven't modified the \"data-root\" in /etc/docker/daemon.json).</p> <ol> <li>Run (<code>docker-compose down</code>) to stop the service.</li> <li>Put the following directories in the same place on the new server:<ul> <li>/var/lib/docker/volumes/dive_addons</li> <li>/var/lib/docker/volumes/dive_girder_assetstore</li> <li>/var/lib/docker/volumes/dive_mongo_db</li> </ul> </li> <li>Tell the new server's docker metadata about the new volumes:<ul> <li><code>docker volume create dive_addons</code></li> <li><code>docker volume create dive_girder_assetstore</code></li> <li><code>docker volume create dive_mongo_db</code></li> </ul> </li> <li>Run <code>docker-compose up</code> on the new server.</li> </ol> <p>The docker official docs have steps for migrating volumes, but they are more complicated because they let you move either named or anonymous volumes. The DIVE volumes are named, so they will likely be easier to move using the steps above.</p>"},{"location":"admin-server-migration/#worker-vm","title":"Worker VM","text":"<p>If using the split services deployment, no user data are stored on the worker VM and thus the worker VM can safely be destroyed and recreated within your GCP project. Follow the worker deployment instructions to deploy and provision the worker. Be sure to provide the correct IP for the web VM. </p> <p>Note that if you do destroy and recreate the worker, you will have to re-download any VIAME addons.</p>"},{"location":"admin-storage/","title":"Cloud Storage Integration","text":"<p>This guide details how to store your imagery in a GCS bucket and mirror that that to your VIAME-Web deployment. This will allow all users to see and use (e.g., annotate, run models on) imagery in the bucket, while ensuring that users cannot delete or modify imagery in the bucket. Annotations are stored in a MongoDB database on the VM.</p>"},{"location":"admin-storage/#setup","title":"Setup","text":"<p>This section expands on the Cloud Storage Integration docs.</p>"},{"location":"admin-storage/#creating-access-credentials","title":"Creating access credentials","text":"<p>Follow the Creating access credentials instructions. You should already have created the service account during deployment. You must create an access key for the same service account that is attached to your VM(s).</p>"},{"location":"admin-storage/#setting-up-cors","title":"Setting up CORS","text":"<p>Confirm that CORS headers are configured for your GCS bucket(s).</p>"},{"location":"admin-storage/#create-assetstore","title":"Create Assetstore","text":"<p>Go to http://localhost:8010/girder#assetstores, and click \u2018Create new Amazon S3 assetstore\u2019. Fill in the options as follows:</p> <ul> <li>Assetstore name: The name of the assetstore. It is recommended to use the same name as the bucket.</li> <li>S3 bucket name The name of the GCS bucket.</li> <li>Path prefix (optional): Leave this blank if you wish to mount the whole bucket.</li> <li>Access key ID: The access key ID from creating your service account access credentials.</li> <li>Secret access key: The secret key from creating your service account access credentials.</li> <li>Service: Enter the GCP service url: <code>https://storage.googleapis.com</code>.</li> <li>Region: The GCP region that your bucket is in. This will likely be <code>us-east4</code>.</li> <li>It is recommended that you check 'Read only' so that users cannot edit the bucket through the VIAME-Web deployment.</li> </ul> <p>Click the 'Create' button to create the assetstore.</p>"},{"location":"admin-storage/#create-local-mount-point","title":"Create local mount point","text":"<p>To make these data broadly accessible to the users of your deployment, create a \u2018Collection\u2019. Go to \u2018Collections\u2019 tab on the Girder page (http://localhost:8010/girder#collections) and click \u2018Create collection\u2019. Make the collection name the same as the bucket. Create a folder within the collection called 'bucket-mount\u2019. Get the \u2018Unique ID\u2019 of the 'bucket-mount' folder by clicking into the folder and clicking the \u2018I\u2019 (info) button. Copy this unique ID.</p> <p>For users to be able to annotate images from the mounted bucket, you must give them edit permissions on the collection. To give a user permission to annotate any images in the collection: </p> <ol> <li>Click into the collection, click 'Actions', and select 'Access control'. </li> <li>Add users as editors as appropriate and select 'Also set permissions on all subfolders to match this collection's permissions'. </li> <li>Click 'Save'.</li> </ol> <p>You can also specify more granular levels of permissions as appropriate.</p>"},{"location":"admin-storage/#import-data","title":"Import data","text":"<p>Now that the mount point has been created, you must 'import' the data from the bucket. Go back to http://localhost:8010/girder#assetstores and click 'Import data' for the desired assetstore. Either a) click the folder icon to browse to the 'bucket-mount' folder or b) enter the unique ID of the 'bucket-mount' folder in the 'Destination ID' box. If you wish to import the whole bucket, leave 'Import path' blank. </p> <p>Click 'Begin import' and the service will begin to import the data so that the imagery and folders in the bucket show up in the collection.</p>"},{"location":"admin-storage/#assetstore-management","title":"Assetstore Management","text":"<p>See these docs and this question for data mirroring guidelines. </p>"},{"location":"admin-storage/#adding-or-deleting-data","title":"Adding or deleting data","text":"<p>Whenever you add new folders and/or imagery to the bucket, you must repeat the 'Import data' step. Currently the mount point can only be kept up to date automatically using Pub/Sub notifications if your server has a public static IP address.</p> <p>Also, without Pub/Sub notifications, you must delete folders from the collection using the VIAME-Web user interface, even after deleting the folders from the bucket. </p>"},{"location":"deployment-access/","title":"Access Instance of VIAME-Web","text":"<p>If you have configured the containers to start when the VM is started, then skip to Create SSH Tunnel. Otherwise, run the startup scripts first.</p>"},{"location":"deployment-access/#run-startup-scripts","title":"Run Startup Scripts","text":"<p>The services (Docker containers) installed on the VM(s) must be running to access the VIAME-Web instance, and thus unless you have configured the containers to start when the VM is started, it is not enough just to turn on the VMs. These commands to start the services can be run from Cloud Shell or a local shell with Google Cloud CLI installed (i.e., a Cloud SDK shell).</p> <p>Note that for other users to run the startup script, they must have permission to run <code>docker-compose</code> on the VM. To allow this, you can add users to the docker group. See manage docker as a non-root user for details.</p>"},{"location":"deployment-access/#default","title":"Default","text":"<pre><code>ZONE=us-east4-c\nINSTANCE_NAME=viame-web\ngcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command=\"/opt/noaa/dive_startup_full.sh\"\n</code></pre>"},{"location":"deployment-access/#split-services","title":"Split Services","text":"<pre><code>ZONE=us-east4-c\nINSTANCE_NAME_WEB=viame-web-web\ngcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\"\n</code></pre> <p>If you wish to also start the worker services:</p> <pre><code>INSTANCE_NAME_WORKER=viame-web-worker\ngcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE --command=\"/opt/noaa/dive_startup_worker.sh\"\n</code></pre>"},{"location":"deployment-access/#create-ssh-tunnel","title":"Create SSH Tunnel","text":"<p>Now that you have provisioned and restarted the VM(s), you can almost access the VIAME-Web instance. Assuming the stack is running, you need to tunnel the 8010 port through SSH. The following command needs to be run from a Cloud SDK shell on your local workstation. Note that you may need to define variables and/or change the variable format.</p> <pre><code>gcloud compute ssh $INSTANCE_NAME --zone=$ZONE -- -N -L 8010:localhost:8010\n</code></pre> <p>If on a Windows, the first time you run this command you likely will get a PuTTY Security Alert popup window asking if you trust the host. Click either 'Accept' (recommended) or 'Connect Once' to be able to access the server.</p> <p>You should now be able to access the VIAME-Web instance (the web service) at http://localhost:8010, as described in the DIVE docs.</p>"},{"location":"deployment-default/","title":"Default Deployment","text":"<p>NOTE: Be sure to first read the General deployment instructions.</p> <p>These instructions are for a single VM with at least one GPU, meaning all operations (annotation, training, etc.) will happen on this VM. For hybrid options, e.g., a low-cost compute VM paired with a worker VM with GPUs, see Split Services.</p>"},{"location":"deployment-default/#create-gcp-resources","title":"Create GCP Resources","text":"<p>To create a single VM for an instance of VIAME-Web, create a VM with at least one GPU using the 'viame-web-noaa-gcp' module. The source path to this module can either be relative (e.g., '../viame-web-noaa-gcp') or an unprefixed <code>github.com</code> URL (e.g., 'github.com/us-amlr/viame-web-noaa-gcp'). Be sure to provide a non-zero value for gpu_count. </p> <p>Click here to download the Terraform code template for a VM for a default deployment. Copy this code into your Terraform config file (e.g., main.tf) and update project-specific values as needed. In the Cloud Shell Terminal, run <code>terraform init</code>, and then <code>terraform apply</code> to create the resources.</p>"},{"location":"deployment-default/#provision-gcp-vm","title":"Provision GCP VM","text":"<p>Once you have created the VM, set variables in your Cloud Shell session that will be used throughout. Then, run a command to download the install script to the VM, make it executable, and then run the install script on the VM. Running this install script may take 10-15 minutes.</p> <pre><code>ZONE=us-east4-c\nINSTANCE_NAME=viame-web\nREPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts\n\ngcloud compute ssh $INSTANCE_NAME --zone=$ZONE \\\n  --command=\"curl -L $REPO_URL/dive_install.sh -o ~/dive_install.sh \\\n  &amp;&amp; chmod +x ~/dive_install.sh \\\n  &amp;&amp; ./dive_install.sh\"\n</code></pre> <p>Because of permissions changes and installing the NVIDIA drivers, the VM must now be restarted. While you can restart the VM from the console, it is generally easiest to run the following from Cloud Shell to 1) restart the VM and 2) run the startup script to pull updated files and spin up the VIAME-Web stack:</p> <pre><code>gcloud compute instances stop $INSTANCE_NAME --zone=$ZONE \\\n  &amp;&amp; gcloud compute instances start $INSTANCE_NAME --zone=$ZONE\n</code></pre> <pre><code>gcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command=\"/opt/noaa/dive_startup_full.sh\"\n</code></pre>"},{"location":"deployment-default/#access-viame-web-deployment","title":"Access VIAME-Web deployment","text":"<p>See Access VIAME-Web</p>"},{"location":"deployment-general/","title":"General","text":""},{"location":"deployment-general/#overview","title":"Overview","text":"<p>These instructions assume that you manage the configuration of your GCP project through Terraform, and specifically that run your Terraform commands through Cloud Shell. Note also that users must have GCP Project Owner or Administrator privileges (e.g., roles/owner).  </p> <p>The subsequent pages will walk you through several steps: creating the virtual machine(s) and other resources in GCP, provisioning the VM(s), and accessing your deployment of VIAME-Web. </p> <p>These instructions work and this deployment is (relatively) straightforward because the VIAME team has created Docker containers for the different services that make up VIAME-Web. Thus, after some VM configuration, we can simply download and run these containers to use spin up our own instance of VIAME-Web. Updating the services is also as simple as pulling down updated versions of the containers.</p> <p>The steps on this page apply to both deployment scenarios. Unless otherwise specified, these commands (and commands in the scenario-specific instructions) are expected to be run from Cloud Shell.</p>"},{"location":"deployment-general/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>Read the Deployment Options Overview and Cloud Deployment Guide - Before you begin . </p> </li> <li> <p>Ensure that the required network changes have been made for your project.</p> </li> <li> <p>Ensure that the Google Cloud CLI tools are installed and configured on your local workstation.</p> </li> </ul>"},{"location":"deployment-general/#create-gcp-resources","title":"Create GCP Resources","text":"<p>Both scenarios require many of the same resources, including: a GCS bucket in which to store imagery that will be connected to your VIAME-Web deployment, a NMFS-approved image, and a service account with sufficient permissions. </p> <p>Click here to download the Terraform code template for general resources. Copy this code into your Terraform config file (e.g., main.tf) and update project-specific values as needed. In the Cloud Shell Terminal, run <code>terraform init</code>, and then <code>terraform apply</code> to create the resources.</p> <p>The architecture of a deployment is below. For additional reference, see the NOAA Fisheries GCP Customer Guide</p> <p></p>"},{"location":"deployment-general/#other-notes","title":"Other Notes","text":"<p>Note that there might be a delay between a VM has been created and/or started, and when you can run an install or startup script. If you get an error, please wait a few minutes and try to run the command again.</p> <p>See here for more information about the required CORS headers for the bucket. </p>"},{"location":"deployment-split/","title":"Split Services Deployment","text":"<p>NOTE: Be sure to first read the General deployment instructions.</p> <p>These instructions are for splitting VIAME-Web web and worker services across two VMs: a compute-only web VM for annotations, and a worker VM with one or more GPUs that can be turned on as needed to run jobs. This is a cost-effective solution, as the (expensive) worker VM is only turned on as needed. See the DIVE docs for more details. For running VIAME-Web on s single VM, see the Default Deployment.</p>"},{"location":"deployment-split/#create-gcp-resources","title":"Create GCP Resources","text":"<p>To create two VMs for a split services instance of VIAME-Web, create two VMs: a web and a worker. The infrastructure of the web and worker VMs should be identical, except that the web node will have no GPU, should have a slightly larger disk capacity, and needs SSH AllowTcpForwarding to be enabled. </p> <p>Click here to download the Terraform code template for the VMs for a split services deployment. Copy this code into your Terraform config file (e.g., main.tf) and update project-specific values as needed. In the Cloud Shell Terminal, run <code>terraform init</code>, and then <code>terraform apply</code> to create the resources.</p>"},{"location":"deployment-split/#provision-gcp-vms","title":"Provision GCP VMs","text":"<p>First, we set the variables in Cloud Shell that will be used throughout. Note that both install scripts require the internal IP of the web node.</p> <pre><code>ZONE=us-east4-c\nINSTANCE_NAME_WEB=viame-web-web\nINSTANCE_NAME_WORKER=viame-web-worker\nREPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts\nWEB_INTERNAL_IP=$(gcloud compute instances describe $INSTANCE_NAME_WEB --zone=$ZONE --format='get(networkInterfaces[0].networkIP)')\n</code></pre>"},{"location":"deployment-split/#web-vm","title":"Web VM","text":"<p>Once the VMs have been created and variables have been set, run the following command to download the install script to the VM, make it executable, and finally run the install script.</p> <pre><code>gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE \\\n  --command=\"curl -L $REPO_URL/dive_install_web.sh -o ~/dive_install_web.sh \\\n  &amp;&amp; chmod +x ~/dive_install_web.sh \\\n  &amp;&amp; ~/dive_install_web.sh $WEB_INTERNAL_IP\"\n</code></pre> <p>You still need to restart the VM to allow permissions changes to take effect. Then, run the startup script for the web node.</p> <pre><code>gcloud compute instances stop $INSTANCE_NAME_WEB --zone=$ZONE \\\n  &amp;&amp; gcloud compute instances start $INSTANCE_NAME_WEB --zone=$ZONE\n</code></pre> <pre><code>gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\"\n</code></pre>"},{"location":"deployment-split/#worker-vm","title":"Worker VM","text":"<p>Next, provision the worker. As above, the following command downloads the install script to the VM, makes it executable, and then runs the install script. Running this install script may take 10-15 minutes.</p> <pre><code>gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE \\\n  --command=\"curl -L $REPO_URL/dive_install.sh -o ~/dive_install.sh \\\n  &amp;&amp; chmod +x ~/dive_install.sh \\\n  &amp;&amp; ~/dive_install.sh -w $WEB_INTERNAL_IP\"\n</code></pre> <p>Because of permissions changes and installing the NVIDIA drivers, the VM must now be restarted. Restart the VM and run the startup script to pull updated files and spin up the VIAME-Web stack:</p> <pre><code>gcloud compute instances stop $INSTANCE_NAME_WORKER --zone=$ZONE \\\n  &amp;&amp; gcloud compute instances start $INSTANCE_NAME_WORKER --zone=$ZONE\n</code></pre> <pre><code>gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE --command=\"/opt/noaa/dive_startup_worker.sh\"\n</code></pre>"},{"location":"deployment-split/#access-viame-web-deployment","title":"Access VIAME-Web deployment","text":"<p>See Access VIAME-Web</p>"},{"location":"deployment-split/#web-and-worker-vm-communication","title":"Web and Worker VM Communication","text":"<p>For the split services to be able to work, the web and worker VMs must be able to communicate. You can confirm this either through either the DIVE API (recommended) or the VMs directly. Before testing the connection, be sure that 1) both the web and worker VMs are on and the services have been started (i.e., the startup scripts have been run) and 2) both VMs have the your viame network tag applied. </p>"},{"location":"deployment-split/#dive-api","title":"DIVE API","text":"<ol> <li>Open the swagger UI at <code>http://{server_url}:{server_port}/api/v1</code> (likely http://localhost:8010/api/v1).</li> <li>Under the 'worker' endpoint, issue a <code>GET /worker/status</code> request. </li> <li>The 'Response Body' section should be a long list of successful connection attempts. If the 'Response Body' values are <code>null</code>, then there is a communication issue.</li> </ol>"},{"location":"deployment-split/#other","title":"Other","text":"<p>SSH into the web VM and check that the VM is listening on at least ports 8010 and 5672. Note that you must have root access to run these commands.</p> <pre><code># check if VM is listening on any ports\n# should list at least 8010 and 5672 as LISTEN\nsudo apt install net-tools #install if necessary \nnetstat -plaunt\n\n# Get the internal IP of the web VM from the third output block if needed\nifconfig\n</code></pre> <p>SSH into the worker VM and check if the VM can make a connection to the web VM on the expected ports. These commands should output a string like <code>Connection to ##.##.##.## 8010 port [tcp/*] succeeded!</code>. If the worker VM cannot make a connection to the web VM, then you will get a 'operation timed out' message.</p> <pre><code>WEB_IP=##.##.##.##\nnc -v -w3 $WEB_IP 8010\nnc -v -w3 $WEB_IP 5672\n</code></pre>"},{"location":"network-changes/","title":"NOAA Fisheries Cloud Network Changes","text":"<p>Deploying an instance of VIAME-Web requires several configuration changes within your GCP project. These changes can be requested by submitting a System Change Request (SCR). </p> <ul> <li> <p>To be able to access this web service, the SSH server's configuration (file: /etc/ssh/sshd_config) needs to include AllowTCPForwarding=yes. However, this is a baseline setting, specifically: \"CIS Benchmarks for Ubuntu Linux 20.04 LTS v1.1.0\" Server Level 2 - 5.3.20 \"Ensure SSH AllowTcpForwarding is disabled\". Thus this config change must be approved by NMFS Change Control Board (CCB) on an per-instance basis. The viame-web-noaa-gcp install scripts change this setting as necessary when the machines are being provisioned, and thus you only need to follow the deployment instructions once you have received approval to use this setting.</p> </li> <li> <p>If you are splitting services between a web and a worker node, you must allow communication between these VMs. In GCP networks are software defined, and thus all traffic is blocked unless a VPC firewall rule is created to allow it, even if it's on the same subnet. Thus, for split services, traffic between ports 8010 (Web traffic) and 5672 (RabbitMQ) within the subnet must be explicitly allowed between the web and worker nodes. In your SCR, request to have the network team add a tag that allows traffic between these ports for the IP addresses in your subnet. Apply this network tag to your web and worker VMs.</p> </li> <li> <p>Private Google Access must be enabled within the project to allow (along with appropriate service account permissions) communication between the VM and buckets. This should now be turned on by default for every project. You can confirm that Private Google Access is enabled for your subnet by following these instructions.</p> </li> </ul>"},{"location":"qwiklabs/","title":"Qwiklabs deployment","text":"<p>These instructions are for users of the 'GCP IaC with Terraform and Cloud Foundation Toolkit' Qwiklab. Note that Qwiklabs does not allow for GPU deployment, and thus Qwiklabs users will only be able to deploy the web VM of a split services deployment.</p>"},{"location":"qwiklabs/#create-gcp-resources","title":"Create GCP Resources","text":"<p>Click here to download the Terraform code template for the Qwiklabs deployment. Copy this code into your Terraform config file (e.g., main.tf) and update project-specific values as needed. In the Cloud Shell Terminal, run <code>terraform init</code>, and then <code>terraform apply</code> to create the resources.</p>"},{"location":"qwiklabs/#provision-gcp-vm","title":"Provision GCP VM","text":"<p>Once you have created the VM, set variables in your Cloud Shell session that will be used throughout. </p> <pre><code>ZONE=us-east4-c\nINSTANCE_NAME_WEB=viame-web-web\nREPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts\nWEB_INTERNAL_IP=$(gcloud compute instances describe $INSTANCE_NAME_WEB --zone=$ZONE --format='get(networkInterfaces[0].networkIP)')\n</code></pre> <p>Install ansible.</p> <pre><code>gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE \\\n  --command=\"sudo apt-get update \\\\\n  &amp;&amp; sudo apt-get install software-properties-common \\\\\n  &amp;&amp; sudo add-apt-repository --yes --update ppa:ansible/ansible \\\\\n  &amp;&amp; sudo apt-get -y install ansible\"\n</code></pre> <p>Now, run a command to download the install script to the VM, make it executable, and then run the install script on the VM.</p> <pre><code>gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE \\\n  --command=\"curl -L $REPO_URL/dive_install_web.sh -o ~/dive_install_web.sh \\\n  &amp;&amp; chmod +x ~/dive_install_web.sh \\\n  &amp;&amp; ~/dive_install_web.sh $WEB_INTERNAL_IP\"\n</code></pre> <p>You still need to restart the VM to allow permissions changes to take effect. Then, run the startup script for the web node.</p> <pre><code>gcloud compute instances stop $INSTANCE_NAME_WEB --zone=$ZONE &amp;&amp; \\\n  gcloud compute instances start $INSTANCE_NAME_WEB --zone=$ZONE\n</code></pre> <pre><code>gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\"\n</code></pre>"},{"location":"qwiklabs/#access-viame-web-deployment","title":"Access VIAME-Web deployment","text":"<p>See Access VIAME-Web</p>"},{"location":"support/","title":"Support","text":"<p>For support regarding the use of VIAME-Web, see the User Guide and Frequently Asked Questions. </p> <p>For support with the deployment an instance of VIAME-Web in the NOAA Fisheries Cloud, there are several options:</p> <ul> <li>For issues with or clarification on the deployment instructions on this site, you can create an issue or contact Sam directly at <code>sam.woodman@noaa.gov</code></li> <li>For issues specifically related to your NOAA Fisheries Cloud GCP project, contact Ed Rodgers at <code>ed.rodgers@noaa.gov</code></li> <li>For general support you can also contact the VIAME-Web team directly at <code>viame-web@kitware.com</code></li> </ul>"}]}